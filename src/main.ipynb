{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleFeedforwardNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size, dropout_rate=0.0):\n",
    "        super(SimpleFeedforwardNN, self).__init__()\n",
    "        \n",
    "        #layers\n",
    "        self.input_layer = nn.Linear(input_size, hidden_size1)\n",
    "        self.hidden_layer1 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.hidden_layer2 = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "        #dropout\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #relu activation function for input layer\n",
    "        x = F.relu(self.input_layer(x))\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        #relu activation function for hidden layer 1\n",
    "        x = F.relu(self.hidden_layer1(x))\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        #log softmax activation function for hidden layer 2\n",
    "        x = F.log_softmax(self.hidden_layer2(x), dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(labels, num_classes):\n",
    "    return F.one_hot(labels, num_classes).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, train_loader, test_loader, epochs=50, loss_name=None, num_classes=3, l1_lambda=0.0):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # if loss function is MSELoss, convert labels to one-hot\n",
    "            if loss_name == 'MSELoss':\n",
    "                labels = F.one_hot(labels, num_classes).float()\n",
    "\n",
    "            #compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            if l1_lambda > 0:\n",
    "                l1_penalty = sum(torch.sum(torch.abs(param)) for param in model.parameters())\n",
    "                loss += l1_lambda * l1_penalty\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        train_losses.append(running_loss / len(train_loader))\n",
    "\n",
    "        # Evaluate the model on the test set\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # if loss function is MSELoss, convert labels to one-hot\n",
    "                if loss_name == 'MSELoss':\n",
    "                    labels = F.one_hot(labels, num_classes).float()\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                if l1_lambda > 0:\n",
    "                    l1_penalty = sum(torch.sum(torch.abs(param)) for param in model.parameters())\n",
    "                    loss += l1_lambda * l1_penalty\n",
    "\n",
    "                test_loss += loss.item()\n",
    "\n",
    "        test_losses.append(test_loss / len(test_loader))\n",
    "\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    return train_losses, test_losses, training_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the data into tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 4\n",
    "hidden_size1 = 10\n",
    "hidden_size2 = 8\n",
    "output_size = 3\n",
    "\n",
    "#model\n",
    "model = SimpleFeedforwardNN(input_size, hidden_size1, hidden_size2, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the different loss functions\n",
    "loss_functions = {\n",
    "    'CrossEntropyLoss': nn.CrossEntropyLoss(),\n",
    "    'MSELoss': nn.MSELoss(),\n",
    "    'NLLLoss': nn.NLLLoss()\n",
    "}\n",
    "\n",
    "#loss records\n",
    "loss_records = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with CrossEntropyLoss...\n",
      "Training with MSELoss...\n",
      "Training with NLLLoss...\n"
     ]
    }
   ],
   "source": [
    "#training the model with different loss functions\n",
    "for loss_name, criterion in loss_functions.items():\n",
    "    print(f\"Training with {loss_name}...\")\n",
    "    \n",
    "    # optimizer\n",
    "    model = SimpleFeedforwardNN(input_size, hidden_size1, hidden_size2, output_size)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # train the model\n",
    "    train_losses, test_losses, _ = train_model(model, criterion, optimizer, train_loader, test_loader, loss_name=loss_name)\n",
    "    \n",
    "    # save the loss records\n",
    "    loss_records[loss_name] = {\n",
    "        'train_losses': train_losses,\n",
    "        'test_losses': test_losses\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss function: CrossEntropyLoss\n",
      "Train loss: 0.2302\n",
      "Test loss: 0.1789\n",
      "\n",
      "Loss function: MSELoss\n",
      "Train loss: 2.1917\n",
      "Test loss: 2.1895\n",
      "\n",
      "Loss function: NLLLoss\n",
      "Train loss: 0.2240\n",
      "Test loss: 0.1603\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for loss_name, losses in loss_records.items():\n",
    "    print(f\"Loss function: {loss_name}\")\n",
    "    print(f\"Train loss: {losses['train_losses'][-1]:.4f}\")\n",
    "    print(f\"Test loss: {losses['test_losses'][-1]:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularization_techniques = {\n",
    "    'No Regularization': {'weight_decay': 0.0, 'dropout_rate': 0.0, 'l1_lambda': 0.0},\n",
    "    'L2 Regularization': {'weight_decay': 0.01, 'dropout_rate': 0.0, 'l1_lambda': 0.0},\n",
    "    'L1 Regularization': {'weight_decay': 0.0, 'dropout_rate': 0.0, 'l1_lambda': 0.01},\n",
    "    'Dropout': {'weight_decay': 0.0, 'dropout_rate': 0.5, 'l1_lambda': 0.0},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with No Regularization...\n",
      "Training with L2 Regularization...\n",
      "Training with L1 Regularization...\n",
      "Training with Dropout...\n"
     ]
    }
   ],
   "source": [
    "loss_records_regularization = {}\n",
    "\n",
    "# train the model with different regularization techniques\n",
    "for reg_name, reg_params in regularization_techniques.items():\n",
    "    print(f\"Training with {reg_name}...\")\n",
    "    \n",
    "    model = SimpleFeedforwardNN(input_size, hidden_size1, hidden_size2, output_size, dropout_rate=reg_params['dropout_rate'])\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=reg_params['weight_decay'])\n",
    "    \n",
    "    train_losses, test_losses, _ = train_model(\n",
    "        model, \n",
    "        criterion, \n",
    "        optimizer, \n",
    "        train_loader, \n",
    "        test_loader, \n",
    "        epochs=50, \n",
    "        loss_name='CrossEntropyLoss',\n",
    "        l1_lambda=reg_params['l1_lambda']\n",
    "    )\n",
    "    \n",
    "    loss_records_regularization[reg_name] = {\n",
    "        'train_losses': train_losses,\n",
    "        'test_losses': test_losses\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization technique: No Regularization\n",
      "Train loss: 0.2886\n",
      "Test loss: 0.2204\n",
      "\n",
      "Regularization technique: L2 Regularization\n",
      "Train loss: 0.3703\n",
      "Test loss: 0.2974\n",
      "\n",
      "Regularization technique: L1 Regularization\n",
      "Train loss: 0.6292\n",
      "Test loss: 0.5707\n",
      "\n",
      "Regularization technique: Dropout\n",
      "Train loss: 0.6772\n",
      "Test loss: 0.4485\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for reg_name, losses in loss_records_regularization.items():\n",
    "    print(f\"Regularization technique: {reg_name}\")\n",
    "    print(f\"Train loss: {losses['train_losses'][-1]:.4f}\")\n",
    "    print(f\"Test loss: {losses['test_losses'][-1]:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_techniques = {\n",
    "    'SGD': {'optimizer': optim.SGD, 'batch_size': 1, 'params': {'lr': 0.01}},\n",
    "    'Batch GD': {'optimizer': optim.SGD, 'batch_size': len(train_loader.dataset), 'params': {'lr': 0.01}},\n",
    "    'Mini-Batch GD': {'optimizer': optim.SGD, 'batch_size': 32, 'params': {'lr': 0.01}},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with SGD...\n",
      "Training with Batch GD...\n",
      "Training with Mini-Batch GD...\n"
     ]
    }
   ],
   "source": [
    "loss_records_optimization = {}\n",
    "\n",
    "for opt_name, opt_config in optimization_techniques.items():\n",
    "    print(f\"Training with {opt_name}...\")\n",
    "    \n",
    "    #update the batch size\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=opt_config['batch_size'], shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=opt_config['batch_size'], shuffle=False)\n",
    "    \n",
    "    model = SimpleFeedforwardNN(input_size, hidden_size1, hidden_size2, output_size)\n",
    "    optimizer = opt_config['optimizer'](model.parameters(), **opt_config['params'])\n",
    "    \n",
    "    train_losses, test_losses, training_time = train_model(\n",
    "        model, \n",
    "        criterion, \n",
    "        optimizer, \n",
    "        train_loader, \n",
    "        test_loader,\n",
    "        epochs=50, \n",
    "        loss_name='CrossEntropyLoss'\n",
    "    )\n",
    "    \n",
    "    loss_records_optimization[opt_name] = {\n",
    "        'train_losses': train_losses,\n",
    "        'test_losses': test_losses,\n",
    "        'training_time': training_time\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization technique: SGD\n",
      "Train loss: 0.0622\n",
      "Test loss: 0.0386\n",
      "Training time: 3.83 seconds\n",
      "\n",
      "Optimization technique: Batch GD\n",
      "Train loss: 1.0594\n",
      "Test loss: 1.0627\n",
      "Training time: 0.13 seconds\n",
      "\n",
      "Optimization technique: Mini-Batch GD\n",
      "Train loss: 0.7414\n",
      "Test loss: 0.7111\n",
      "Training time: 0.33 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for opt_name, losses in loss_records_optimization.items():\n",
    "    print(f\"Optimization technique: {opt_name}\")\n",
    "    print(f\"Train loss: {losses['train_losses'][-1]:.4f}\")\n",
    "    print(f\"Test loss: {losses['test_losses'][-1]:.4f}\")\n",
    "    print(f\"Training time: {losses['training_time']:.2f} seconds\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ¿Cuál es la principal innovación de la arquitectura Transformer?\n",
    "\n",
    "A diferencia de los modelos RNN o CNN, los cuales usaban una artquitectura de encoder-decoder y tienen muchas limitantes de rendimiento y dependencia, la arquitectura transformer busca obtener una atención mayor al contexto dado por medio del Self-Attention. Esto permite que todas las posiciones o tokens de la secuencia puedan atender a cualquier otra posición de la misma, obteniendo así un mejor alcance y mayor comprensión del contexto. Esto permite que sea altamente paralelizadle, por lo que este tipo de arquitecturas proveen una mayor eficiencia computacional.\n",
    "\n",
    "2. ¿Cómo funciona el mecanismo de atención del scaled dot-product?\n",
    "\n",
    "Este mecanismo funciona por medio de tres matrices de entrada: Kyes, Values y Queries. Dadas estas matrices, se obtiene el producto punto de Queries y de Keys, para luego dividirlo entre la raíz cuadrada de la dimensionalidad de Keys. Luego de aplicar softmax al resultado, se obtiene un conjunto de ponderaciones que, combinadas con su respectivo Values, devuelve un vector el cual indica la importancia o relevancia de cada Value.\n",
    "\n",
    "3. ¿Por qué se utiliza la atención de múltiples cabezales en Transformer?\n",
    "\n",
    "La razón de utilizar múltiples cabezales para la atención es debido a que, de esta forma es posible paralelizar diferentes perspectivas de el mismo conjunto de información. Debido a este tipo de arquitectura, los transformers pueden explorar diferentes perspectivas de la información al mismo tiempo. \n",
    "\n",
    "4. ¿Cómo se incorporan los positional encodings en el modelo Transformer?\n",
    "5. ¿Cuáles son algunas aplicaciones de la arquitectura Transformer más allá de la machine translation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
