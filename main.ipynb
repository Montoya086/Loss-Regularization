{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleFeedforwardNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size, dropout_rate=0.0):\n",
    "        super(SimpleFeedforwardNN, self).__init__()\n",
    "        \n",
    "        #layers\n",
    "        self.input_layer = nn.Linear(input_size, hidden_size1)\n",
    "        self.hidden_layer1 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.hidden_layer2 = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "        #dropout\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #relu activation function for input layer\n",
    "        x = F.relu(self.input_layer(x))\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        #relu activation function for hidden layer 1\n",
    "        x = F.relu(self.hidden_layer1(x))\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        #log softmax activation function for hidden layer 2\n",
    "        x = F.log_softmax(self.hidden_layer2(x), dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(labels, num_classes):\n",
    "    return F.one_hot(labels, num_classes).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, train_loader, test_loader, epochs=50, loss_name=None, num_classes=3, l1_lambda=0.0):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # if loss function is MSELoss, convert labels to one-hot\n",
    "            if loss_name == 'MSELoss':\n",
    "                labels = F.one_hot(labels, num_classes).float()\n",
    "\n",
    "            #compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            if l1_lambda > 0:\n",
    "                l1_penalty = sum(torch.sum(torch.abs(param)) for param in model.parameters())\n",
    "                loss += l1_lambda * l1_penalty\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        train_losses.append(running_loss / len(train_loader))\n",
    "\n",
    "        # Evaluate the model on the test set\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # if loss function is MSELoss, convert labels to one-hot\n",
    "                if loss_name == 'MSELoss':\n",
    "                    labels = F.one_hot(labels, num_classes).float()\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                if l1_lambda > 0:\n",
    "                    l1_penalty = sum(torch.sum(torch.abs(param)) for param in model.parameters())\n",
    "                    loss += l1_lambda * l1_penalty\n",
    "\n",
    "                test_loss += loss.item()\n",
    "\n",
    "        test_losses.append(test_loss / len(test_loader))\n",
    "\n",
    "    return train_losses, test_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the data into tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 4\n",
    "hidden_size1 = 10\n",
    "hidden_size2 = 8\n",
    "output_size = 3\n",
    "\n",
    "#model\n",
    "model = SimpleFeedforwardNN(input_size, hidden_size1, hidden_size2, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the different loss functions\n",
    "loss_functions = {\n",
    "    'CrossEntropyLoss': nn.CrossEntropyLoss(),\n",
    "    'MSELoss': nn.MSELoss(),\n",
    "    'NLLLoss': nn.NLLLoss()\n",
    "}\n",
    "\n",
    "#loss records\n",
    "loss_records = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with CrossEntropyLoss...\n",
      "Training with MSELoss...\n",
      "Training with NLLLoss...\n"
     ]
    }
   ],
   "source": [
    "#training the model with different loss functions\n",
    "for loss_name, criterion in loss_functions.items():\n",
    "    print(f\"Training with {loss_name}...\")\n",
    "    \n",
    "    # optimizer\n",
    "    model = SimpleFeedforwardNN(input_size, hidden_size1, hidden_size2, output_size)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # train the model\n",
    "    train_losses, test_losses = train_model(model, criterion, optimizer, train_loader, test_loader, loss_name=loss_name)\n",
    "    \n",
    "    # save the loss records\n",
    "    loss_records[loss_name] = {\n",
    "        'train_losses': train_losses,\n",
    "        'test_losses': test_losses\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss function: CrossEntropyLoss\n",
      "Train loss: 0.3131\n",
      "Test loss: 0.2539\n",
      "\n",
      "Loss function: MSELoss\n",
      "Train loss: 2.1955\n",
      "Test loss: 2.1918\n",
      "\n",
      "Loss function: NLLLoss\n",
      "Train loss: 0.3063\n",
      "Test loss: 0.2592\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for loss_name, losses in loss_records.items():\n",
    "    print(f\"Loss function: {loss_name}\")\n",
    "    print(f\"Train loss: {losses['train_losses'][-1]:.4f}\")\n",
    "    print(f\"Test loss: {losses['test_losses'][-1]:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularization_techniques = {\n",
    "    'No Regularization': {'weight_decay': 0.0, 'dropout_rate': 0.0, 'l1_lambda': 0.0},\n",
    "    'L2 Regularization': {'weight_decay': 0.01, 'dropout_rate': 0.0, 'l1_lambda': 0.0},\n",
    "    'L1 Regularization': {'weight_decay': 0.0, 'dropout_rate': 0.0, 'l1_lambda': 0.01},\n",
    "    'Dropout': {'weight_decay': 0.0, 'dropout_rate': 0.5, 'l1_lambda': 0.0},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with No Regularization...\n",
      "Training with L2 Regularization...\n",
      "Training with L1 Regularization...\n",
      "Training with Dropout...\n"
     ]
    }
   ],
   "source": [
    "loss_records_regularization = {}\n",
    "\n",
    "# train the model with different regularization techniques\n",
    "for reg_name, reg_params in regularization_techniques.items():\n",
    "    print(f\"Training with {reg_name}...\")\n",
    "    \n",
    "    model = SimpleFeedforwardNN(input_size, hidden_size1, hidden_size2, output_size, dropout_rate=reg_params['dropout_rate'])\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=reg_params['weight_decay'])\n",
    "    \n",
    "    train_losses, test_losses = train_model(\n",
    "        model, \n",
    "        criterion, \n",
    "        optimizer, \n",
    "        train_loader, \n",
    "        test_loader, \n",
    "        epochs=50, \n",
    "        loss_name='CrossEntropyLoss',\n",
    "        l1_lambda=reg_params['l1_lambda']\n",
    "    )\n",
    "    \n",
    "    loss_records_regularization[reg_name] = {\n",
    "        'train_losses': train_losses,\n",
    "        'test_losses': test_losses\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization technique: No Regularization\n",
      "Train loss: 0.3042\n",
      "Test loss: 0.2383\n",
      "\n",
      "Regularization technique: L2 Regularization\n",
      "Train loss: 0.4275\n",
      "Test loss: 0.3867\n",
      "\n",
      "Regularization technique: L1 Regularization\n",
      "Train loss: 0.5935\n",
      "Test loss: 0.5386\n",
      "\n",
      "Regularization technique: Dropout\n",
      "Train loss: 0.6930\n",
      "Test loss: 0.4721\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for reg_name, losses in loss_records_regularization.items():\n",
    "    print(f\"Regularization technique: {reg_name}\")\n",
    "    print(f\"Train loss: {losses['train_losses'][-1]:.4f}\")\n",
    "    print(f\"Test loss: {losses['test_losses'][-1]:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
