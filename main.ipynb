{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleFeedforwardNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(SimpleFeedforwardNN, self).__init__()\n",
    "        \n",
    "        #layers\n",
    "        self.input_layer = nn.Linear(input_size, hidden_size1)\n",
    "        self.hidden_layer1 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.hidden_layer2 = nn.Linear(hidden_size2, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #relu activation function for input layer\n",
    "        x = F.relu(self.input_layer(x))\n",
    "        \n",
    "        #relu activation function for hidden layer 1\n",
    "        x = F.relu(self.hidden_layer1(x))\n",
    "        \n",
    "        #log softmax activation function for hidden layer 2\n",
    "        x = F.log_softmax(self.hidden_layer2(x), dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(labels, num_classes):\n",
    "    return F.one_hot(labels, num_classes).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, train_loader, test_loader, epochs=50, loss_name=None, num_classes=3):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # if loss function is MSELoss, convert labels to one-hot\n",
    "            if loss_name == 'MSELoss':\n",
    "                labels = F.one_hot(labels, num_classes).float()\n",
    "\n",
    "            #compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        train_losses.append(running_loss / len(train_loader))\n",
    "\n",
    "        # Evaluate the model on the test set\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # if loss function is MSELoss, convert labels to one-hot\n",
    "                if loss_name == 'MSELoss':\n",
    "                    labels = F.one_hot(labels, num_classes).float()\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_loss += loss.item()\n",
    "\n",
    "        test_losses.append(test_loss / len(test_loader))\n",
    "\n",
    "    return train_losses, test_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the data into tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 4\n",
    "hidden_size1 = 10\n",
    "hidden_size2 = 8\n",
    "output_size = 3\n",
    "\n",
    "#model\n",
    "model = SimpleFeedforwardNN(input_size, hidden_size1, hidden_size2, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the different loss functions\n",
    "loss_functions = {\n",
    "    'CrossEntropyLoss': nn.CrossEntropyLoss(),\n",
    "    'MSELoss': nn.MSELoss(),\n",
    "    'NLLLoss': nn.NLLLoss()\n",
    "}\n",
    "\n",
    "#loss records\n",
    "loss_records = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with CrossEntropyLoss...\n",
      "Training with MSELoss...\n",
      "Training with NLLLoss...\n"
     ]
    }
   ],
   "source": [
    "#training the model with different loss functions\n",
    "for loss_name, criterion in loss_functions.items():\n",
    "    print(f\"Training with {loss_name}...\")\n",
    "    \n",
    "    # optimizer\n",
    "    model = SimpleFeedforwardNN(input_size, hidden_size1, hidden_size2, output_size)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # train the model\n",
    "    train_losses, test_losses = train_model(model, criterion, optimizer, train_loader, test_loader, loss_name=loss_name)\n",
    "    \n",
    "    # save the loss records\n",
    "    loss_records[loss_name] = {\n",
    "        'train_losses': train_losses,\n",
    "        'test_losses': test_losses\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss function: CrossEntropyLoss\n",
      "Train loss: 0.3771\n",
      "Test loss: 0.3249\n",
      "\n",
      "Loss function: MSELoss\n",
      "Train loss: 2.1942\n",
      "Test loss: 2.1959\n",
      "\n",
      "Loss function: NLLLoss\n",
      "Train loss: 0.2591\n",
      "Test loss: 0.1989\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for loss_name, losses in loss_records.items():\n",
    "    print(f\"Loss function: {loss_name}\")\n",
    "    print(f\"Train loss: {losses['train_losses'][-1]:.4f}\")\n",
    "    print(f\"Test loss: {losses['test_losses'][-1]:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
